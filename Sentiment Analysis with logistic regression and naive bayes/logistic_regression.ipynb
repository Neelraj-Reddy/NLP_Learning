{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /home/neelraj-\n",
      "[nltk_data]     reddy/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/neelraj-\n",
      "[nltk_data]     reddy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt \n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords      \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "1. Remove stop words\n",
    "2. Remove URL's and tag's\n",
    "3. remove punctuations\n",
    "4. Then others like retweets using RT, #'s, stock market tickers like $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    tweet = re.sub('\\$\\w*',\"\",tweet)      #-----> removes any word starting with dollar sign\n",
    "\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)     #-----> removes RT   and following whitespace\n",
    "\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)   #-----> remove hyperlink\n",
    "\n",
    "    tweet = re.sub(r'#', '', tweet)   #-----> remove # from the text\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@borapls it was white washed dude to the Polaroid effect :-(\n",
      "['white', 'wash', 'dude', 'polaroid', 'effect', ':-(']\n"
     ]
    }
   ],
   "source": [
    "print(train_neg[510])\n",
    "print(preprocess_tweet(train_neg[510]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \n",
    "    # tweets: a list of tweets\n",
    "    # ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n",
    "   \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in preprocess_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    \n",
    "    # freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
    "    \n",
    "    return freqs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 13065\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[]\n",
    "for key in freqs.keys():\n",
    "    keys.append(key[0])\n",
    "\n",
    "keys=list(set(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['squad', 1, 0],\n",
       " ['buy', 11, 17],\n",
       " ['back', 98, 122],\n",
       " ['jumma', 5, 0],\n",
       " ['saw', 19, 19],\n",
       " ['chara', 0, 1],\n",
       " ['ouat', 0, 1],\n",
       " ['sheet', 0, 1],\n",
       " ['msg', 2, 1],\n",
       " ['nisrina', 0, 1],\n",
       " ['katekyn', 0, 1],\n",
       " ['bucket', 0, 1],\n",
       " ['roller', 2, 0],\n",
       " ['avail', 10, 13],\n",
       " ['bigtim', 0, 1],\n",
       " ['d:', 0, 6],\n",
       " ['summertim', 0, 1],\n",
       " ['ale', 1, 0],\n",
       " ['omw', 0, 1],\n",
       " ['anythin', 0, 1],\n",
       " ['triathlet', 1, 0],\n",
       " ['tuesday', 7, 4],\n",
       " ['carrot', 0, 1],\n",
       " ['happi', 211, 25],\n",
       " ['nb', 1, 0],\n",
       " ['happybirthdayemilybett', 1, 0],\n",
       " ['south', 2, 2],\n",
       " ['awkward', 2, 1],\n",
       " ['denim', 1, 1],\n",
       " ['jax', 1, 0],\n",
       " ['belfast', 1, 1],\n",
       " ['noo', 0, 5],\n",
       " ['lion', 1, 3],\n",
       " ['ohaha', 1, 0],\n",
       " ['wahoo', 1, 0],\n",
       " ['cashback', 1, 0],\n",
       " ['influenti', 0, 1],\n",
       " ['immov', 0, 1],\n",
       " ['afso', 0, 1],\n",
       " ['announc', 1, 3],\n",
       " ['pakighinabi', 0, 1],\n",
       " ['sn', 1, 0],\n",
       " ['simpson', 0, 1],\n",
       " ['accid', 0, 2],\n",
       " ['carmel', 1, 0],\n",
       " ['bilal', 0, 1],\n",
       " ['jelli', 1, 0],\n",
       " ['mornin', 2, 0],\n",
       " ['neighbor', 0, 1],\n",
       " ['pokiri', 1, 0],\n",
       " ['idownload', 1, 0],\n",
       " ['actuallythough', 1, 0],\n",
       " ['luna', 0, 1],\n",
       " ['eczema', 1, 0],\n",
       " ['zap', 0, 1],\n",
       " ['pepsi', 1, 0],\n",
       " ['gtg', 0, 1],\n",
       " ['90210', 1, 0],\n",
       " ['alot', 2, 3],\n",
       " ['awel', 0, 1],\n",
       " ['sem', 0, 2],\n",
       " ['expect', 6, 5],\n",
       " [\"shamuon'\", 0, 1],\n",
       " ['blame', 1, 3],\n",
       " ['dreamer', 1, 0],\n",
       " ['yeeeah', 1, 0],\n",
       " ['urself', 2, 2],\n",
       " ['york', 2, 1],\n",
       " ['thrill', 2, 0],\n",
       " ['sum', 1, 2],\n",
       " ['tour', 8, 7],\n",
       " ['sat', 1, 3],\n",
       " ['aaj', 0, 1],\n",
       " ['internet', 2, 9],\n",
       " ['soamaz', 1, 0],\n",
       " ['master', 3, 1],\n",
       " ['toptravelcentar', 1, 0],\n",
       " ['spade', 0, 1],\n",
       " ['7/29', 1, 0],\n",
       " ['349', 0, 1],\n",
       " ['occas', 1, 0],\n",
       " ['punt', 1, 0],\n",
       " ['repay', 1, 0],\n",
       " ['wru', 0, 1],\n",
       " ['puppi', 1, 3],\n",
       " ['cosplay', 4, 0],\n",
       " ['Ô∏èbea', 1, 0],\n",
       " ['agenc', 0, 1],\n",
       " ['shade', 1, 3],\n",
       " ['laribuggi', 0, 1],\n",
       " ['130', 0, 1],\n",
       " ['troy', 1, 0],\n",
       " ['quan', 1, 0],\n",
       " ['smu', 1, 0],\n",
       " ['shepherd', 0, 2],\n",
       " ['yello', 1, 0],\n",
       " ['straight', 0, 4],\n",
       " ['gvb', 1, 0],\n",
       " ['barkada', 1, 1],\n",
       " ['hyungwon', 0, 2],\n",
       " ['french', 5, 4],\n",
       " ['ng', 2, 4],\n",
       " ['poorli', 0, 6],\n",
       " ['sunburn', 1, 1],\n",
       " ['sissi', 0, 1],\n",
       " ['cjradacomateada', 2, 0],\n",
       " ['wing', 1, 3],\n",
       " ['yayi', 2, 0],\n",
       " ['yawn', 1, 0],\n",
       " ['indiemus', 5, 10],\n",
       " ['contemporari', 0, 1],\n",
       " ['7df89150', 0, 1],\n",
       " ['whdjwksja', 0, 1],\n",
       " ['roll', 5, 4],\n",
       " ['enemi', 2, 0],\n",
       " ['üòÖ', 1, 0],\n",
       " ['grandad', 0, 1],\n",
       " ['mp3', 1, 0],\n",
       " ['heard', 9, 6],\n",
       " ['wft', 0, 1],\n",
       " ['tulan', 1, 0],\n",
       " ['jordanian', 1, 0],\n",
       " ['scouser', 1, 0],\n",
       " ['splendour', 0, 1],\n",
       " ['oop', 5, 4],\n",
       " ['hanaaa', 0, 1],\n",
       " ['christ', 1, 1],\n",
       " ['phl', 1, 0],\n",
       " [\"s'okay\", 1, 0],\n",
       " ['centuri', 1, 0],\n",
       " ['tri', 44, 65],\n",
       " ['news', 25, 21],\n",
       " ['wasp', 0, 2],\n",
       " ['journorequest', 1, 0],\n",
       " ['irresist', 2, 0],\n",
       " ['walk', 12, 7],\n",
       " ['unprepar', 0, 1],\n",
       " ['swallow', 0, 1],\n",
       " ['lile', 1, 0],\n",
       " ['thestruggleisr', 0, 1],\n",
       " ['ah', 13, 18],\n",
       " ['crime', 0, 1],\n",
       " ['7-3', 0, 1],\n",
       " ['dear', 17, 11],\n",
       " ['bad', 18, 73],\n",
       " ['advisori', 0, 1],\n",
       " ['chan', 1, 0],\n",
       " ['alliter', 1, 0],\n",
       " ['jasmingarrick', 0, 2],\n",
       " ['judi', 1, 0],\n",
       " ['mochamichel', 1, 0],\n",
       " ['somerset', 0, 1],\n",
       " ['nicknam', 1, 1],\n",
       " ['invest', 2, 2],\n",
       " ['beirut', 2, 0],\n",
       " ['modern', 1, 1],\n",
       " ['trish', 1, 0],\n",
       " ['apexi', 3, 0],\n",
       " ['thencerest', 2, 0],\n",
       " ['thermo', 0, 1],\n",
       " ['again', 0, 1],\n",
       " ['nat', 0, 2],\n",
       " ['siddi', 1, 0],\n",
       " ['meaning', 0, 1],\n",
       " ['warsaw', 44, 0],\n",
       " ['waduh', 0, 1],\n",
       " ['creta', 1, 0],\n",
       " ['clandestin', 0, 1],\n",
       " ['üòç', 2, 1],\n",
       " ['chickmt', 0, 1],\n",
       " ['beato', 0, 1],\n",
       " ['hous', 7, 16],\n",
       " ['\\U000fec00', 1, 0],\n",
       " ['premium', 0, 2],\n",
       " ['disk', 1, 0],\n",
       " ['0', 3, 1],\n",
       " ['similar', 1, 4],\n",
       " ['content', 4, 1],\n",
       " ['sƒôxxx√ø', 1, 0],\n",
       " ['wba', 0, 1],\n",
       " ['legend', 0, 3],\n",
       " ['spain', 2, 4],\n",
       " ['wayn', 0, 2],\n",
       " ['nutshel', 1, 0],\n",
       " ['arc', 1, 0],\n",
       " ['tragic', 0, 1],\n",
       " ['yulin', 0, 1],\n",
       " ['climb', 1, 3],\n",
       " ['yoohoo', 1, 0],\n",
       " ['doomsday', 0, 1],\n",
       " ['karain', 2, 0],\n",
       " ['ayemso', 0, 1],\n",
       " ['round', 2, 3],\n",
       " ['nemanja', 0, 1],\n",
       " ['railway', 1, 1],\n",
       " ['gestur', 1, 0],\n",
       " ['1-0', 0, 1],\n",
       " ['peru', 1, 0],\n",
       " ['minhyuk', 0, 1],\n",
       " ['access', 3, 2],\n",
       " ['safer', 0, 1],\n",
       " ['rajud', 0, 1],\n",
       " ['garlic', 1, 0],\n",
       " ['aamir', 1, 0],\n",
       " ['tera', 1, 0],\n",
       " ['z', 1, 5],\n",
       " ['wobbl', 1, 1],\n",
       " ['technic', 1, 0],\n",
       " ['590', 0, 1],\n",
       " ['utub', 1, 0],\n",
       " ['socialreward', 0, 1],\n",
       " ['snot', 0, 1],\n",
       " ['sakho', 0, 1],\n",
       " ['taco', 2, 2],\n",
       " ['dubai', 0, 3],\n",
       " ['m4', 0, 1],\n",
       " ['didnt', 2, 24],\n",
       " ['hotel', 5, 8],\n",
       " ['smashingbook', 1, 0],\n",
       " ['y≈´j≈ç-cosplay', 1, 0],\n",
       " ['sarcasm', 2, 0],\n",
       " ['auguri', 1, 0],\n",
       " ['mel', 0, 1],\n",
       " ['mtg', 0, 1],\n",
       " ['sexchat', 1, 0],\n",
       " ['scroll', 0, 1],\n",
       " ['overwhelmingli', 1, 0],\n",
       " ['experttradesmen', 1, 0],\n",
       " ['easi', 9, 6],\n",
       " ['915', 0, 1],\n",
       " ['treat', 5, 5],\n",
       " ['foampit', 1, 0],\n",
       " ['sheen', 1, 0],\n",
       " ['braid', 0, 2],\n",
       " ['thrive', 1, 0],\n",
       " ['footbal', 1, 3],\n",
       " ['crack', 1, 2],\n",
       " ['üë±üèΩ', 1, 0],\n",
       " ['mamaya', 1, 0],\n",
       " ['walangmakakapigilsakin', 1, 0],\n",
       " ['across', 0, 4],\n",
       " ['leia', 1, 0],\n",
       " ['kendal', 0, 1],\n",
       " ['wakeupgop', 0, 1],\n",
       " ['hotaisndonwyvauwjoqhsjsnaihsuswtf', 0, 1],\n",
       " ['snapchatm', 0, 6],\n",
       " ['nutella', 0, 1],\n",
       " ['üëç', 1, 1],\n",
       " ['doggi', 0, 1],\n",
       " ['wherev', 1, 0],\n",
       " ['fashionfriday', 1, 0],\n",
       " ['charger', 2, 1],\n",
       " ['cardio', 1, 0],\n",
       " ['pamuk', 1, 0],\n",
       " ['eri', 0, 1],\n",
       " ['nhi', 2, 1],\n",
       " ['four', 5, 4],\n",
       " ['nicola', 1, 0],\n",
       " ['prob', 5, 5],\n",
       " ['ameen', 1, 0],\n",
       " ['albay', 1, 0],\n",
       " ['bother', 2, 3],\n",
       " ['timog', 1, 0],\n",
       " ['voucher', 2, 1],\n",
       " ['mar', 1, 0],\n",
       " ['either', 11, 13],\n",
       " ['qualiti', 3, 1],\n",
       " ['grandma', 0, 1],\n",
       " ['jealou', 4, 9],\n",
       " ['finddjderek', 0, 1],\n",
       " ['chocolatey', 1, 0],\n",
       " ['cont', 0, 2],\n",
       " ['easier', 2, 3],\n",
       " ['sadkaay', 1, 0],\n",
       " ['looong', 0, 1],\n",
       " ['fail', 5, 10],\n",
       " ['luckyyi', 0, 1],\n",
       " ['karibumombasa', 1, 0],\n",
       " ['ive', 1, 11],\n",
       " ['lechon', 0, 1],\n",
       " ['photoset', 3, 0],\n",
       " ['snap', 4, 3],\n",
       " ['practic', 3, 1],\n",
       " ['offic', 8, 7],\n",
       " ['expel', 2, 0],\n",
       " ['blameshoghicp', 0, 1],\n",
       " ['goddamn', 0, 1],\n",
       " ['polish', 1, 0],\n",
       " ['thq', 1, 0],\n",
       " ['kid', 18, 20],\n",
       " ['be-shak', 1, 0],\n",
       " ['31st', 2, 1],\n",
       " ['glare', 0, 1],\n",
       " ['omigod', 0, 1],\n",
       " ['hashtag', 5, 1],\n",
       " ['coldplay', 1, 1],\n",
       " ['garret', 1, 0],\n",
       " ['ensur', 1, 0],\n",
       " ['cruis', 0, 1],\n",
       " ['libmysqlclient-dev', 0, 1],\n",
       " ['uy', 0, 1],\n",
       " ['went', 12, 32],\n",
       " ['sone', 0, 1],\n",
       " ['yet', 13, 33],\n",
       " ['ingat', 1, 0],\n",
       " ['doug', 3, 0],\n",
       " ['adulthood', 1, 0],\n",
       " ['icloud', 0, 1],\n",
       " ['powi', 1, 0],\n",
       " ['mojo', 0, 1],\n",
       " ['war', 0, 2],\n",
       " ['polic', 0, 4],\n",
       " ['cram', 0, 1],\n",
       " ['occasion', 0, 1],\n",
       " ['chai', 1, 0],\n",
       " ['shud', 1, 0],\n",
       " ['lendal', 1, 0],\n",
       " ['loveu', 1, 0],\n",
       " ['piec', 0, 4],\n",
       " ['mb', 1, 1],\n",
       " ['benchmark', 1, 0],\n",
       " ['followfriday', 25, 0],\n",
       " ['llaollao', 0, 1],\n",
       " ['sj', 0, 3],\n",
       " ['turkey', 1, 1],\n",
       " ['dunt', 0, 1],\n",
       " ['hse', 0, 1],\n",
       " ['thnk', 1, 0],\n",
       " ['baechyyi', 0, 1],\n",
       " ['tempt', 0, 2],\n",
       " ['sapiosexu', 0, 1],\n",
       " ['etdi', 1, 0],\n",
       " ['doabl', 0, 1],\n",
       " ['instagood', 2, 0],\n",
       " ['hd', 0, 2],\n",
       " ['nate', 2, 2],\n",
       " ['sleepingwithsiren', 0, 1],\n",
       " ['worthi', 1, 1],\n",
       " ['atlanti', 1, 0],\n",
       " ['kareem', 1, 0],\n",
       " ['x39', 1, 0],\n",
       " ['srijith', 1, 0],\n",
       " ['differ', 11, 10],\n",
       " ['shitti', 1, 1],\n",
       " ['tierd', 0, 2],\n",
       " ['pardon', 1, 0],\n",
       " ['thx', 15, 1],\n",
       " ['monsoon', 1, 0],\n",
       " ['2013', 0, 1],\n",
       " ['xma', 2, 0],\n",
       " ['roomi', 1, 0],\n",
       " ['tomhiddleston', 1, 0],\n",
       " ['lafayett', 0, 2],\n",
       " ['takfaham', 0, 1],\n",
       " ['wuppert', 1, 0],\n",
       " ['lmaoo', 2, 0],\n",
       " ['stabl', 1, 1],\n",
       " ['shek', 0, 1],\n",
       " ['suger', 0, 1],\n",
       " ['retain', 0, 1],\n",
       " ['apb', 0, 2],\n",
       " ['irand', 0, 1],\n",
       " ['‚òÄ', 2, 0],\n",
       " ['daisi', 1, 0],\n",
       " ['routin', 4, 0],\n",
       " ['symphoni', 0, 1],\n",
       " ['confisc', 0, 1],\n",
       " ['27juli', 1, 0],\n",
       " ['wouldv', 0, 1],\n",
       " ['finland', 1, 3],\n",
       " ['sherep', 0, 1],\n",
       " ['busier', 0, 1],\n",
       " ['prosecco', 1, 0],\n",
       " ['axio', 1, 0],\n",
       " ['hors', 2, 3],\n",
       " [\"selena'\", 0, 1],\n",
       " ['crypt', 1, 0],\n",
       " ['mister', 2, 0],\n",
       " ['dumb', 2, 2],\n",
       " ['__', 1, 2],\n",
       " ['samosa', 1, 0],\n",
       " ['oitnb', 1, 1],\n",
       " ['loveyeah', 1, 0],\n",
       " ['macci', 0, 1],\n",
       " ['nowher', 2, 3],\n",
       " ['mnwreeen', 1, 0],\n",
       " ['letshavecocktailsafternuclai', 1, 0],\n",
       " ['webcam', 2, 2],\n",
       " ['memor', 0, 1],\n",
       " ['hayley', 2, 0],\n",
       " ['obstacl', 1, 0],\n",
       " ['mbulelo', 0, 1],\n",
       " ['x36', 1, 0],\n",
       " ['sourc', 3, 0],\n",
       " ['sp', 0, 1],\n",
       " ['umpfff', 1, 0],\n",
       " ['jumpgiant', 1, 0],\n",
       " ['bakwa', 0, 1],\n",
       " ['debacl', 1, 0],\n",
       " ['cnt', 0, 1],\n",
       " ['gail', 1, 0],\n",
       " ['horribl', 1, 9],\n",
       " ['onward', 0, 1],\n",
       " ['manzano', 0, 1],\n",
       " ['outfit', 3, 5],\n",
       " ['eidwithgrof', 0, 1],\n",
       " ['road', 5, 1],\n",
       " ['expedia', 0, 1],\n",
       " ['reaction', 2, 1],\n",
       " ['pigeon', 1, 1],\n",
       " ['nd', 1, 1],\n",
       " ['klappertart', 0, 1],\n",
       " ['plu', 4, 4],\n",
       " ['haix', 0, 2],\n",
       " ['565', 0, 1],\n",
       " ['lazi', 2, 3],\n",
       " ['wong', 0, 1],\n",
       " ['contectu', 1, 0],\n",
       " ['gaon', 0, 1],\n",
       " ['dash', 0, 1],\n",
       " ['australia', 5, 3],\n",
       " ['lini', 0, 1],\n",
       " ['ikea', 0, 1],\n",
       " ['dmme', 1, 2],\n",
       " ['üíò', 3, 2],\n",
       " ['stockholm', 1, 1],\n",
       " ['keepitloc', 2, 0],\n",
       " ['rebound', 0, 1],\n",
       " ['word', 20, 16],\n",
       " ['shawti', 0, 1],\n",
       " ['steroid', 1, 0],\n",
       " ['activ', 5, 6],\n",
       " ['scan', 1, 0],\n",
       " ['hediy', 1, 0],\n",
       " ['unless', 3, 4],\n",
       " ['891', 1, 0],\n",
       " ['koi', 0, 1],\n",
       " ['exp', 0, 1],\n",
       " ['wh', 0, 1],\n",
       " ['biooo', 0, 1],\n",
       " ['dirt', 1, 0],\n",
       " ['gan', 1, 0],\n",
       " ['memem', 0, 2],\n",
       " ['con-gradu', 1, 0],\n",
       " ['tear', 1, 3],\n",
       " ['unstan', 0, 1],\n",
       " ['lollipop', 1, 0],\n",
       " ['bitch', 3, 11],\n",
       " ['window', 7, 6],\n",
       " ['tame', 2, 0],\n",
       " ['peel', 2, 1],\n",
       " ['antagonist', 0, 1],\n",
       " ['turtl', 0, 2],\n",
       " ['everyth', 13, 17],\n",
       " ['purti', 0, 1],\n",
       " ['siddiqu', 1, 0],\n",
       " ['et', 0, 3],\n",
       " ['heartburn', 0, 1],\n",
       " ['iphon', 7, 5],\n",
       " ['junmyeon', 0, 1],\n",
       " ['jsl', 1, 0],\n",
       " ['kept', 0, 2],\n",
       " ['identifi', 1, 0],\n",
       " ['updat', 13, 11],\n",
       " ['can', 0, 1],\n",
       " ['commit', 4, 2],\n",
       " ['tompolo', 1, 0],\n",
       " ['proceed', 1, 0],\n",
       " ['comedi', 1, 0],\n",
       " ['lowbat', 0, 1],\n",
       " ['real_liam_payn', 0, 1],\n",
       " ['oppayaa', 1, 0],\n",
       " ['peyton', 1, 0],\n",
       " ['hahaha', 14, 11],\n",
       " ['mythic', 1, 0],\n",
       " ['sepanx', 0, 1],\n",
       " ['5:30', 1, 2],\n",
       " ['supposedli', 1, 1],\n",
       " ['recharg', 1, 0],\n",
       " ['20/1', 0, 1],\n",
       " ['basic', 4, 2],\n",
       " ['knew', 8, 6],\n",
       " ['joim', 1, 0],\n",
       " ['acorn', 1, 0],\n",
       " ['erni', 0, 1],\n",
       " ['extraordinari', 1, 0],\n",
       " ['kca', 1, 0],\n",
       " ['üòµ', 0, 1],\n",
       " ['eat', 6, 27],\n",
       " [\"week'\", 1, 0],\n",
       " ['masud', 0, 1],\n",
       " ['extens', 0, 1],\n",
       " ['confus', 3, 8],\n",
       " ['89.9', 1, 0],\n",
       " ['b4', 0, 1],\n",
       " ['done', 54, 24],\n",
       " ['graduat', 3, 3],\n",
       " ['upward', 1, 0],\n",
       " ['ivi', 1, 0],\n",
       " ['courgett', 1, 0],\n",
       " [\"school'\", 1, 0],\n",
       " ['aigoo', 0, 1],\n",
       " ['short', 7, 11],\n",
       " ['poldi', 1, 0],\n",
       " ['peni', 0, 1],\n",
       " ['bv', 0, 1],\n",
       " ['thick', 1, 0],\n",
       " ['averag', 2, 0],\n",
       " ['m31', 1, 0],\n",
       " ['cardi', 1, 0],\n",
       " ['depart', 2, 0],\n",
       " ['comput', 5, 6],\n",
       " ['uk', 4, 9],\n",
       " ['xialan', 1, 0],\n",
       " ['confer', 1, 2],\n",
       " ['ghanton', 1, 0],\n",
       " ['boah', 0, 1],\n",
       " ['olivia', 0, 2],\n",
       " ['responsibilti', 1, 0],\n",
       " ['haestarr', 0, 1],\n",
       " ['mirror', 3, 1],\n",
       " ['üçµ', 0, 2],\n",
       " ['accomplish', 1, 0],\n",
       " ['meadowhal', 0, 1],\n",
       " ['hygien', 1, 0],\n",
       " ['aj', 1, 1],\n",
       " ['choroo', 1, 0],\n",
       " ['flippin', 1, 0],\n",
       " ['diz', 0, 1],\n",
       " ['bahari', 1, 0],\n",
       " ['imac', 0, 1],\n",
       " ['format', 1, 0],\n",
       " ['zayniscomingback', 0, 3],\n",
       " ['ohgod', 0, 1],\n",
       " ['plan', 27, 17],\n",
       " ['everett', 1, 0],\n",
       " ['thunderstorm', 1, 0],\n",
       " ['festiv', 8, 2],\n",
       " ['twine', 1, 0],\n",
       " ['gue', 0, 1],\n",
       " ['mash', 0, 1],\n",
       " ['glue', 0, 1],\n",
       " ['typa', 0, 3],\n",
       " ['dev', 0, 2],\n",
       " ['3point', 1, 0],\n",
       " ['burger', 3, 2],\n",
       " ['bb17', 0, 1],\n",
       " ['blackfli', 1, 0],\n",
       " ['mini', 0, 1],\n",
       " ['finish', 17, 14],\n",
       " ['decid', 4, 10],\n",
       " ['chau', 1, 0],\n",
       " ['in-sensit', 0, 1],\n",
       " ['woot', 2, 0],\n",
       " ['ov', 0, 2],\n",
       " ['allgoodthingsk', 1, 0],\n",
       " ['bond', 4, 0],\n",
       " ['emc', 1, 0],\n",
       " ['üéÄ', 0, 1],\n",
       " ['11:00', 1, 0],\n",
       " ['freya', 1, 0],\n",
       " ['raini', 1, 2],\n",
       " ['aameen', 2, 0],\n",
       " ['scotlandismass', 1, 0],\n",
       " ['4:13', 1, 0],\n",
       " ['üôà', 1, 0],\n",
       " ['cola', 1, 0],\n",
       " ['tyre', 2, 0],\n",
       " ['twice', 1, 5],\n",
       " ['broadcast', 1, 6],\n",
       " ['devolut', 1, 0],\n",
       " ['unev', 1, 0],\n",
       " ['hay', 1, 8],\n",
       " ['pcupgrad', 0, 1],\n",
       " ['syria', 0, 2],\n",
       " ['kar', 1, 0],\n",
       " ['barcelona', 0, 2],\n",
       " ['checkout', 2, 0],\n",
       " ['idol', 3, 2],\n",
       " ['li', 0, 1],\n",
       " ['yosh', 1, 0],\n",
       " ['bulb', 5, 0],\n",
       " ['beij', 1, 0],\n",
       " ['1,300', 0, 1],\n",
       " ['tenyai', 1, 0],\n",
       " [\"mum'\", 1, 0],\n",
       " ['jummamubarak', 1, 0],\n",
       " ['sauc', 3, 0],\n",
       " ['kanina', 1, 2],\n",
       " ['complain', 3, 2],\n",
       " ['wwooo', 1, 0],\n",
       " ['neng', 0, 1],\n",
       " ['cue', 1, 0],\n",
       " ['yer', 2, 0],\n",
       " ['austin', 1, 1],\n",
       " ['nostalgia', 1, 0],\n",
       " ['subtl', 1, 0],\n",
       " ['commiss', 0, 1],\n",
       " ['bcoz', 0, 1],\n",
       " ['yk', 0, 1],\n",
       " ['gegu', 0, 1],\n",
       " ['lansi', 0, 1],\n",
       " ['victori', 0, 1],\n",
       " ['17', 4, 0],\n",
       " ['helpinggroupdm', 1, 0],\n",
       " ['persona', 1, 1],\n",
       " ['lolo', 0, 1],\n",
       " ['matt', 6, 2],\n",
       " ['node', 0, 1],\n",
       " ['gusto', 0, 1],\n",
       " ['uu', 1, 2],\n",
       " ['justget', 0, 1],\n",
       " ['milkshak', 0, 1],\n",
       " ['osad', 0, 1],\n",
       " ['record', 5, 6],\n",
       " ['forrit', 1, 0],\n",
       " ['kang', 0, 1],\n",
       " ['aishhh', 0, 1],\n",
       " ['triangl', 2, 0],\n",
       " ['cooler', 0, 1],\n",
       " ['summon', 1, 0],\n",
       " ['sohappyrightnow', 1, 0],\n",
       " ['owli', 1, 0],\n",
       " ['pea', 0, 1],\n",
       " ['tight', 2, 1],\n",
       " ['magnific', 2, 0],\n",
       " ['chariti', 1, 0],\n",
       " ['incent', 1, 0],\n",
       " ['hbm', 0, 1],\n",
       " ['alrd', 1, 0],\n",
       " ['110', 2, 0],\n",
       " ['bencher', 1, 0],\n",
       " ['üö≤', 2, 0],\n",
       " ['kaha', 0, 1],\n",
       " ['africa', 1, 2],\n",
       " ['roommat', 1, 1],\n",
       " ['keven', 1, 0],\n",
       " ['wed', 5, 4],\n",
       " ['coveral', 0, 1],\n",
       " ['block', 3, 11],\n",
       " ['lineup', 1, 0],\n",
       " ['relev', 1, 0],\n",
       " ['ocean', 0, 4],\n",
       " ['dancer', 0, 1],\n",
       " ['640', 1, 0],\n",
       " ['khatam', 0, 2],\n",
       " ['juan', 1, 1],\n",
       " ['homework', 1, 2],\n",
       " ['wider', 0, 1],\n",
       " ['chloe', 2, 2],\n",
       " ['initi', 4, 1],\n",
       " ['thnkyouuu', 0, 1],\n",
       " ['caaannnttt', 0, 1],\n",
       " [\"i't\", 0, 1],\n",
       " ['particularli', 2, 0],\n",
       " ['gunderson', 1, 0],\n",
       " ['belong', 1, 0],\n",
       " ['forget', 17, 8],\n",
       " ['9:48', 0, 1],\n",
       " ['advis', 1, 0],\n",
       " ['angek', 0, 1],\n",
       " ['tonight', 20, 24],\n",
       " ['irrit', 1, 0],\n",
       " ['famili', 19, 9],\n",
       " ['hold', 6, 9],\n",
       " ['pedal', 1, 0],\n",
       " ['blood', 2, 5],\n",
       " ['greatest', 2, 0],\n",
       " ['earl', 1, 0],\n",
       " ['locat', 0, 7],\n",
       " ['protein', 0, 1],\n",
       " ['thanq', 2, 0],\n",
       " ['eid', 2, 4],\n",
       " ['fake', 2, 4],\n",
       " ['sweeeti', 1, 0],\n",
       " ['fsa', 1, 0],\n",
       " ['quick', 7, 5],\n",
       " ['alhamdullilah', 1, 0],\n",
       " ['motor', 0, 1],\n",
       " ['rattl', 0, 1],\n",
       " ['kasi', 0, 3],\n",
       " ['siriu', 0, 1],\n",
       " ['charg', 3, 4],\n",
       " ['ljp', 0, 1],\n",
       " ['zehr', 0, 1],\n",
       " ['829', 0, 1],\n",
       " ['snapm', 2, 4],\n",
       " ['view', 16, 7],\n",
       " ['younger', 4, 2],\n",
       " ['prisss', 0, 1],\n",
       " ['russia', 2, 0],\n",
       " ['craft', 2, 1],\n",
       " ['1', 75, 26],\n",
       " ['prioriti', 1, 0],\n",
       " ['photobomb', 1, 0],\n",
       " ['okaaay', 0, 1],\n",
       " ['butter', 2, 1],\n",
       " [\"u'll\", 2, 0],\n",
       " ['equal', 2, 0],\n",
       " ['bitin', 0, 1],\n",
       " ['wolverin', 0, 1],\n",
       " ['babygirl', 1, 0],\n",
       " ['serbia', 0, 1],\n",
       " ['bug', 3, 3],\n",
       " ['kindli', 4, 0],\n",
       " ['hulk', 1, 3],\n",
       " ['physiqu', 1, 0],\n",
       " ['bilbao', 0, 1],\n",
       " ['facebook', 5, 4],\n",
       " ['ruth', 0, 1],\n",
       " ['sake', 0, 1],\n",
       " ['tom', 5, 4],\n",
       " [\"tonight'\", 2, 0],\n",
       " ['stun', 4, 1],\n",
       " ['tl', 2, 8],\n",
       " ['ferdou', 0, 1],\n",
       " ['engin', 2, 1],\n",
       " ['arma', 1, 0],\n",
       " ['haiss', 0, 1],\n",
       " ['boredom', 0, 1],\n",
       " ['subj', 0, 1],\n",
       " ['manila', 0, 3],\n",
       " ['chain', 1, 1],\n",
       " ['61', 0, 1],\n",
       " ['braindot', 11, 0],\n",
       " ['buckl', 0, 1],\n",
       " ['pastri', 2, 0],\n",
       " ['persuad', 0, 1],\n",
       " ['deliveri', 3, 3],\n",
       " ['bikini', 3, 0],\n",
       " ['workshop', 3, 1],\n",
       " ['howdo', 0, 1],\n",
       " ['üê∂', 1, 0],\n",
       " ['coyot', 0, 1],\n",
       " ['armor', 0, 2],\n",
       " ['boot', 2, 4],\n",
       " ['aaahh', 0, 1],\n",
       " ['woman', 4, 5],\n",
       " ['morefuninthephilippin', 1, 0],\n",
       " ['glitch', 0, 1],\n",
       " ['ave', 0, 1],\n",
       " ['wali', 0, 1],\n",
       " ['fani', 0, 1],\n",
       " ['scale', 1, 1],\n",
       " ['mandela', 1, 0],\n",
       " ['aditya', 0, 1],\n",
       " ['shine', 3, 1],\n",
       " ['anyonnee', 0, 1],\n",
       " ['murder', 1, 0],\n",
       " ['thnx', 1, 0],\n",
       " ['yup', 6, 4],\n",
       " ['mask', 3, 0],\n",
       " ['steel', 1, 2],\n",
       " ['salon', 1, 0],\n",
       " [\"mine'\", 0, 1],\n",
       " ['shell', 1, 1],\n",
       " ['144p', 0, 1],\n",
       " ['downstair', 1, 0],\n",
       " ['lyin', 0, 1],\n",
       " ['hiondsh', 0, 1],\n",
       " ['msged', 1, 0],\n",
       " ['squash', 1, 0],\n",
       " ['pwrfl', 1, 0],\n",
       " ['sth', 0, 1],\n",
       " ['wise', 1, 0],\n",
       " ['layer', 1, 0],\n",
       " ['chilton', 0, 1],\n",
       " ['inflat', 0, 1],\n",
       " ['brief', 1, 0],\n",
       " ['steven', 0, 2],\n",
       " [\"california'\", 0, 1],\n",
       " ['borderland', 1, 0],\n",
       " ['pong', 1, 0],\n",
       " ['cheung', 1, 0],\n",
       " ['338', 0, 1],\n",
       " ['dota', 3, 1],\n",
       " ['uta', 1, 0],\n",
       " ['struck', 0, 1],\n",
       " ['silent', 0, 1],\n",
       " ['kati', 2, 3],\n",
       " ['yvett', 1, 0],\n",
       " ['benadryl', 0, 1],\n",
       " ['strawberri', 0, 1],\n",
       " ['cosi', 1, 0],\n",
       " ['loneliest', 1, 0],\n",
       " ['stalk', 2, 3],\n",
       " ['reynoldsgrl', 0, 1],\n",
       " ['inuyasha', 0, 2],\n",
       " ['liv', 2, 0],\n",
       " ['drew', 3, 0],\n",
       " ['tc', 1, 0],\n",
       " ['75', 2, 0],\n",
       " ['talkmobil', 1, 0],\n",
       " ['karen', 2, 0],\n",
       " ['cuddl', 1, 8],\n",
       " ['distract', 1, 1],\n",
       " ['socialmedia@dpdgroup.co.uk', 1, 0],\n",
       " ['emili', 3, 0],\n",
       " ['envi', 0, 1],\n",
       " [\"foxy'\", 1, 0],\n",
       " ['saddest', 0, 2],\n",
       " ['smi', 0, 1],\n",
       " ['soprano', 1, 0],\n",
       " ['nite', 3, 0],\n",
       " ['pin', 4, 2],\n",
       " ['jhezz', 0, 1],\n",
       " ['append', 0, 1],\n",
       " ['fyi', 1, 1],\n",
       " ['bonasio', 1, 0],\n",
       " ['mooorn', 1, 0],\n",
       " ['creas', 0, 1],\n",
       " ['loveyoutilltheendcart', 0, 1],\n",
       " ['upset', 1, 11],\n",
       " ['devast', 0, 1],\n",
       " ['colorpencil', 1, 0],\n",
       " ['seatmat', 0, 1],\n",
       " ['frend', 1, 0],\n",
       " ['irrespons', 0, 1],\n",
       " ['hoxton', 1, 0],\n",
       " ['ee', 1, 1],\n",
       " ['congratul', 15, 2],\n",
       " ['centrifug', 1, 0],\n",
       " ['creepi', 0, 2],\n",
       " ['95', 1, 0],\n",
       " ['nfinit', 0, 1],\n",
       " ['selfee', 1, 0],\n",
       " ['climatechang', 0, 5],\n",
       " ['besid', 2, 0],\n",
       " ['countless', 0, 1],\n",
       " ['juicebro', 1, 0],\n",
       " ['sponsor', 4, 3],\n",
       " ['tweeti', 1, 0],\n",
       " ['constant', 1, 0],\n",
       " ['baz', 1, 0],\n",
       " ['stair', 0, 1],\n",
       " ['steal', 0, 1],\n",
       " ['airlin', 1, 0],\n",
       " ['hurray', 1, 0],\n",
       " ['d;', 0, 1],\n",
       " ['pi', 1, 1],\n",
       " ['keep', 68, 34],\n",
       " ['shropshir', 1, 0],\n",
       " ['harriet', 0, 1],\n",
       " ['hungri', 2, 19],\n",
       " ['carli', 1, 0],\n",
       " [\"eye'\", 0, 1],\n",
       " ['ang', 2, 8],\n",
       " ['novemb', 0, 1],\n",
       " ['physiotherapi', 1, 0],\n",
       " ['fabian', 0, 2],\n",
       " ['cycl', 6, 2],\n",
       " ['x24', 1, 0],\n",
       " ['dosent', 0, 1],\n",
       " ['keeno', 0, 1],\n",
       " ['progrmr', 1, 0],\n",
       " ['bose', 0, 1],\n",
       " ['wknd', 1, 0],\n",
       " ['jean', 1, 2],\n",
       " ['73', 0, 2],\n",
       " ['nlb', 0, 1],\n",
       " ['fback', 27, 0],\n",
       " ['swap', 0, 1],\n",
       " ['happycustom', 1, 0],\n",
       " ['ai', 1, 0],\n",
       " ['collagen', 1, 0],\n",
       " ['beti', 0, 4],\n",
       " ['choiceinternationalartist', 9, 5],\n",
       " ['paul', 2, 1],\n",
       " ['pension', 0, 1],\n",
       " ['merci', 1, 1],\n",
       " ['frequentfly', 1, 0],\n",
       " ['star', 9, 4],\n",
       " ['2hr', 0, 1],\n",
       " ['pictur', 12, 17],\n",
       " ['zant', 0, 1],\n",
       " ['stan', 1, 1],\n",
       " ['manteca', 0, 1],\n",
       " ['station', 4, 3],\n",
       " ['youv', 1, 0],\n",
       " ['teas', 1, 3],\n",
       " ['tud', 1, 0],\n",
       " ['reliev', 1, 0],\n",
       " ['hoki', 1, 0],\n",
       " ['mayor', 0, 1],\n",
       " ['kapatidkongpogi', 0, 1],\n",
       " ['trivia', 0, 1],\n",
       " ['chao', 1, 0],\n",
       " ['writer', 0, 1],\n",
       " ['higher', 0, 1],\n",
       " ['sinc', 15, 35],\n",
       " ['among', 5, 1],\n",
       " ['nva', 1, 0],\n",
       " ['bundl', 1, 0],\n",
       " ['hitter', 1, 0],\n",
       " ['town', 5, 14],\n",
       " ['jayci', 1, 0],\n",
       " ['nakakapikon', 0, 1],\n",
       " ['loung', 1, 1],\n",
       " ['bud', 2, 1],\n",
       " ['boy', 21, 18],\n",
       " ['photo', 19, 15],\n",
       " ['rakyat', 1, 0],\n",
       " [\"do't\", 0, 1],\n",
       " ['petit', 0, 1],\n",
       " [\"they'll\", 6, 0],\n",
       " ['bapak', 0, 1],\n",
       " ['swiftma', 1, 0],\n",
       " ['hiii', 2, 0],\n",
       " ['gotten', 2, 1],\n",
       " ['alter', 1, 1],\n",
       " ['macauley', 1, 0],\n",
       " ['banat', 1, 0],\n",
       " ['joint', 1, 0],\n",
       " ['laro', 1, 0],\n",
       " ['welder', 0, 1],\n",
       " ['ludlow', 1, 0],\n",
       " ['minion', 2, 3],\n",
       " ['pake', 1, 0],\n",
       " ['thesi', 0, 11],\n",
       " ['flickr', 1, 0],\n",
       " ['apolog', 4, 3],\n",
       " ['wagga', 2, 0],\n",
       " ['hunger', 1, 0],\n",
       " ['x22', 1, 0],\n",
       " ['typo', 1, 2],\n",
       " ['gran', 1, 2],\n",
       " ['pet', 2, 2],\n",
       " ['pa-copi', 0, 1],\n",
       " ['opossum', 1, 0],\n",
       " ['1pm', 0, 2],\n",
       " ['twitterfollowerswhatsup', 1, 0],\n",
       " ['bantim', 1, 0],\n",
       " ['urgent', 3, 0],\n",
       " ['ytb', 1, 0],\n",
       " ['arent', 0, 1],\n",
       " ['ku', 1, 0],\n",
       " ['morninggg', 1, 0],\n",
       " ['version', 8, 5],\n",
       " ['board', 5, 1],\n",
       " ['arch', 0, 1],\n",
       " ['orcalov', 0, 1],\n",
       " ['pthht', 1, 0],\n",
       " ['11', 5, 4],\n",
       " ['certainli', 4, 2],\n",
       " ['4thwin', 0, 4],\n",
       " ['brewproject', 1, 0],\n",
       " ['thi', 1, 0],\n",
       " ['nath', 1, 0],\n",
       " ['yaap', 1, 0],\n",
       " ['postiv', 2, 0],\n",
       " ['warlock', 1, 0],\n",
       " ['ndabenhl', 0, 1],\n",
       " ['gorefiend', 1, 0],\n",
       " ['tanisha', 1, 0],\n",
       " ['histori', 2, 2],\n",
       " ['mayday', 0, 1],\n",
       " ['ohmygod', 0, 1],\n",
       " ['jasper', 0, 1],\n",
       " ['1000', 1, 1],\n",
       " ['guilti', 0, 1],\n",
       " ['bade', 0, 1],\n",
       " ['cynic', 2, 0],\n",
       " ['um', 1, 0],\n",
       " ['sembuh', 1, 0],\n",
       " ['andromeda', 0, 1],\n",
       " ['moviemarathon', 1, 0],\n",
       " ['boat', 2, 0],\n",
       " ['wkend', 3, 0],\n",
       " ['artworkbyli', 1, 0],\n",
       " ['6-5', 1, 0],\n",
       " ['compliment', 1, 0],\n",
       " ['cafe', 0, 2],\n",
       " ['luxembourg', 1, 0],\n",
       " ['facil', 0, 1],\n",
       " ['white', 4, 7],\n",
       " ['lana', 0, 2],\n",
       " ['ykr', 1, 0],\n",
       " ['assum', 0, 2],\n",
       " ['resid', 2, 0],\n",
       " ['00962778381', 1, 0],\n",
       " ['posey', 1, 0],\n",
       " ['heathrow', 0, 1],\n",
       " ['overnight', 1, 0],\n",
       " ['mtr', 1, 0],\n",
       " ['romant', 2, 0],\n",
       " ['stock', 3, 3],\n",
       " ['yeyi', 0, 1],\n",
       " ['dd', 1, 1],\n",
       " ['mashaket', 0, 1],\n",
       " ['morocco', 1, 0],\n",
       " ['k3g', 0, 1],\n",
       " ['jumma_mubarik', 1, 0],\n",
       " ['journo', 0, 1],\n",
       " ['p90x', 1, 0],\n",
       " ['traffic', 2, 5],\n",
       " ['uber', 2, 3],\n",
       " ['analyz', 1, 0],\n",
       " [\"mark'\", 0, 1],\n",
       " ['üòã', 0, 1],\n",
       " ['ikaw', 1, 0],\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "data = []\n",
    "\n",
    "# loop through our selected words\n",
    "for word in keys:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    \n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = preprocess_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "       \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "         \n",
    "        if (word,1.0) in freqs:\n",
    "            # increment the word count for the positive label 1\n",
    "            x[0,1] += freqs[(word,1.0)]\n",
    "        if(word,0.0) in freqs:\n",
    "            # increment the word count for the negative label 0\n",
    "            x[0,2] += freqs[(word,0.0)]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 3)\n",
      "(8000, 1)\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "\n",
    "    h = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "   \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -(np.dot(y.T,np.log(h))+np.dot((1-y).T,np.log(1-h)))/m\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - alpha*(np.dot(x.T,h-y))/m\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.195138.\n",
      "The resulting vector of weights is [7e-08, 0.00054223, -0.00054312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7595/1638565074.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "\n",
    "print(f\"The cost after training is {J:.6f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8879634]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# check your own sentiment\n",
    "my_tweet = 'I love machine learning :)'\n",
    "y_pred_temp = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_pred_temp)\n",
    "\n",
    "if y_pred_temp > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\" \n",
    "    test_x: a list of tweets\n",
    "    test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    accuracy = np.sum(np.squeeze(test_y) == np.squeeze(np.asarray(y_hat)))/len(test_y)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9965\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "[1.] [[0.49884816]]\n",
      "[1.] [[0.48216204]]\n",
      "[1.] [[0.48216204]]\n",
      "[1.] [[0.48216204]]\n",
      "[1.] [[0.49669377]]\n",
      "[1.] [[0.48163997]]\n",
      "[0.] [[0.50103336]]\n"
     ]
    }
   ],
   "source": [
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        # print('THE TWEET IS:', x)\n",
    "        # print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print(y, y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
